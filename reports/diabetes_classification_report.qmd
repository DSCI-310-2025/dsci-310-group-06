---
title: "Predicting the Risk of Diabetes using Logistic Regression"
authors: 
  - name: Nicholas Tam
    affiliations:
      - ref: ubc
  - name: Dua Khan
    affiliations:
      - ref: ubc
  - name: Kaylee Li
    affiliations:
      - ref: ubc
  - name: Luke Huang
    affiliations:
      - ref: ubc
affiliations:
  - id: ubc
    name: University of British Columbia, Vancouver, BC, Canada, V6T 1Z4
format: 
  html:
    embed-resources: true
    toc: true
    toc-depth: 3
    editor: visual
    bibliography: /home/rstudio/work/reports/references.bib
execute:
  echo: false
  warning: false
  error: false
  eval: true
engine: knitr
---

## 1. Summary

We attempt to develop a logistic regression model to predict whether a patient has diabetes or not using the [Diabetes Health Indicators](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators) dataset sourced from the UCI machine learning repository [@CDC2023]. We employ Random Over-Sampling Examples (ROSE) to balance the data and a tuned Least Absolute Shrinkage and Selection Operator (LASSO) regression model to classify patients who are at high risk of developing diabetes, using 5 out of the 21 risk factors provided in the dataset. Recall was used to measure the classifier's performance, as the consequences of false negatives would be more severe than false positives for this task. The area under the receiver operating characteristic (ROC) curve (AUC) was also chosen to evaluate the model's effectiveness in distinguishing between the two classes compared to random guessing.

## 2. Introduction

Diabetes is a chronic condition characterized by high blood sugar levels, resulting from excess buildup of glucose in the bloodstream [@Mayo2024]. Diabetes is linked to a variety of complications, including retinopathy, cardiovascular disease, stroke, and increased susceptibility to infections [@Papatheodorou2018]. Individuals with diabetes often experience significant reductions in their years of healthy life [@Ong2023]. In 2021, diabetes-related conditions resulted in over 2 million deaths worldwide and the prevalence of diabetes continues to rise, with its growth only accelerating in the 21st century [@WHO2024]. The true mortality rate may be higher than current estimates suggest, as many cases of diabetes go undiagnosed [@Stokes2017]. However, advancements in screening and detection methods have the potential to reduce the number of undiagnosed cases [@Fang2022]. As such, accurate diagnosis in the early stages is crucial since interventions can be administered to prevent the progression of diabetes [@Mayo2024]. This project aims to develop a classification model to predict diabetes status based on various health indicators. By using data preprocessing strategies, we seek to improve the accuracy of diabetes detection using publicly available health datasets. Ultimately, the goal is to answer the following question: **Can we develop a classification model that can predict whether a person will have diabetes more accurately than random guessing?**

The dataset used in this project is sourced from the [CDC Diabetes Health Indicators dataset on the UCI machine learning repository](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators) [@CDC2023], which contains various demographic and lifestyle-related features that may influence the likelihood of an individual developing diabetes. The dataset contains 23 features, of which 21 can be used as predictors in a classification model. The total 23 features are:

-   `ID`: Patient identification number. This feature was removed from the publicly available dataset.
-   `Diabetes_binary`: Diabetes/pre-diabetes (1) or no diabetes (0). This is the target feature.
-   `HighBP`: High blood pressure (1) or not (0)
-   `HighChol`: High cholesterol (1) or not (0)
-   `CholCheck`: Cholesterol check in past 5 years (1) or no check (0)
-   `BMI`: Body mass index
-   `Smoker`: Have smoked at least 100 cigarettes in lifetime (1) or not (0)
-   `Stroke`: Had a stroke in the past (1) or not (0)
-   `HeartDiseaseorAttack`: Had coronary heart disease or myocardial infarction (1) or not (0)
-   `PhysActivity`: Physical activity in the last 30 days (1) or not (0)
-   `Fruits`: Consume fruit 1 or more times per day (1) or not (0)
-   `Veggies`: Consume vegetables 1 or more times per day (1) or not (0)
-   `HvyAlcoholConsump`: Having more than 14 drinks per day for adult men and 7 drinks for women, yes (1) or no (0)
-   `AnyHealthcare`: Have any kind of health care coverage (1) or not (0)
-   `NoDocbcCost`: Could not see a doctor in the past 12 months due to cost (1) or not (0)
-   `GenHlth`: General health rating on scale of 1 - 5
    -   1 = Excellent
    -   2 = Very good
    -   3 = Good
    -   4 = Fair
    -   5 = Poor
-   `MentHlth`: Number of days where mental health was not good in the last 30 days (1 - 30)
-   `PhysHlth`: Number of days where physical health was not good in the last 30 days (1 - 30)
-   `DiffWalk`: Serious difficulty walking or climbing stairs (1) or not (0)
-   `Sex`: Male (1) or female (0)
-   `Age`: Age based on 13-level scale (See codebook \_AGEG5YR for more information)
    -   1 = Age 18-24
    -   9 = 60-64
    -   13 = 80 or older
-   `Education`: Education level based on 6-level scale:
    -   1 = Never attended school/only kindergarten
    -   2 = Grades 1 through 8
    -   3 = Grades 9 through 11
    -   4 = Grade 12 or GED
    -   5 = College 1 year to 3 years
    -   6 = College 4 years or more
-   `Income`: Income based on 8-level scale (See codebook INCOME2 for more information)
    -   1 = Less than \$10,000
    -   5 = Less than \$35,000
    -   8 = \$75,000 or more

The primary objective is to classify individuals into diabetic/high risk of diabetes (`Diabetes_binary = 1`) or non-diabetic (`Diabetes_binary = 0`) categories using predictive modelling.

## 3. Method and Results

The project follows a structured approach to data preparation, exploration, and classification modeling.

*Analysis workflow:*

First, the dataset is obtained from an [external source](https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators) and loaded into R. Then, the raw dataset is inspected for completeness and correctness. This includes checking for missing and unique values in each feature. Categorical variables (e.g. age, smoking status, high blood pressure) are converted into factors to facilitate the analysis.

Moreover, the dataset is highly imbalanced, with more non-diabetic cases than diabetic ones. To address this, the Random Over-Sampling Examples (ROSE) technique is applied to generate synthetic data points to balance the dataset.

Visualizations (bar plots, density plots) are generated to explore the relationships between health indicators and diabetes status. Trends between factors such as `BMI` and `HighBP` with the target variable `Diabetes_binary` are examined. The dataset is split into 75% training data and 25% testing data to build and evaluate the LASSO regression model. According to @Sivakumar2024, the most commonly used train-test splits in the literature are 70:30 and 80:20. However, using too little or too much training data can lead to issues such as underfitting or overfitting. As such, we chose a 75:25 split as a reasonable compromise that has also demonstrated high accuracy in logistic regression machine learning models (@Sivakumar2024). The results of our analysis are visualised as an ROC curve and a confusion matrix.

### 3.1. Loading Data from Original Source on the Web

The packages used in this analysis are `tidyverse`, `tidymodels`, `glmnet`, `patchwork`, `ROSE`, and `vcd`. The dataset of interest can be acquired from the source by running `dataset_download.py` located in the `~/work/src/` directory. This script will fetch the raw dataset (UCI ID: 891) from the [UC Irvine machine learning repository](https://archive.ics.uci.edu/) and then write the result into a .csv file (`cdc_diabetes_health_indicators.csv`) located in the `~/work/data/raw/` directory.

```{r}
# Importing required packages for analysis. Suppress warnings and startup messages the first time libraries are loaded
suppressPackageStartupMessages({
  suppressWarnings(library(tidyverse)) # Data wrangling and visualization
})
```

### 3.2. Preprocessing: Wrangling, Cleaning, and Balancing Data from Original Format

*Analysis Workflow:*

All features in the dataset are checked for the following attributes:

-   `NA_Count`: Number of "NA" values within each variable; if they exist, they would need to be replaced or removed.
-   `Distinct_Count`: Number of possible values for each variable; primarily to check if the variables are numerical, categorical or binary (only 2 possible values).
-   `Current_Data_Type`: Current data type for each variable; primarily to ensure that they are in the appropriate format we require, as the data type may be lost during `read_csv()`.

```{r}
#| label: tbl-diabetes1
#| tbl-cap: Summary of Missing Values, Distinct Counts of each Variable, and Data Types in the Raw Diabetes Dataset

checking_raw_matrix <- readRDS("/home/rstudio/work/output/checking_raw_matrix.RDS")

knitr::kable(as.data.frame(t(checking_raw_matrix)))
```

Given the initial check, the following observations can be made using @tbl-diabetes1:

-   None of the columns have `NA` values.
-   `BMI` is the only numerical variable, with the rest being categorical or binary.
-   All variables are treated as `double` in the original dataset, and thus every variable except for `BMI` will need to be converted to the factor data type.

We aim to balance the dataset for our machine learning model to reduce bias towards the majority class, improve the model's ability to generalize to unseen data, and increase model training efficiency.

```{r}
#| label: tbl-diabetes2
#| tbl-cap: Class Distribution of Diabetes_binary in the Raw Diabetes Dataset

target_result <- read_csv("/home/rstudio/work/output/target_result.csv", show_col_types = FALSE)

# Proportion and percentage of individuals who do not have diabetes (Table 2)
prop_no_diabetes2 <- target_result %>% filter(Diabetes_binary == 0) %>% pull(Proportion)
percent_no_diabetes2 <- round(prop_no_diabetes2*100)

knitr::kable(target_result)
```

In the original `cdc_diabetes_health_indicators.csv` dataset, approximately `r percent_no_diabetes2`% of individuals do not have diabetes, resulting in heavily imbalanced classes (@tbl-diabetes2).

To address this, we use the `ROSE()` function to create a balanced version of the dataset to form `balanced_raw_diabetes_df` by undersampling the majority class and oversampling the minority class. The size of `balanced_raw_diabetes_df` will be equal to that of the original `cdc_diabetes_health_indicators.csv`.

```{r}
#| label: tbl-diabetes3
#| tbl-cap: Class Distribution of Diabetes_binary in the Balanced Dataset

balanced_target_result <- read_csv("/home/rstudio/work/output/balanced_target_result.csv", show_col_types = FALSE)

# Proportion and percent of individuals who do not have diabetes (Table 3)
prop_no_diabetes3 <- balanced_target_result %>% filter(Diabetes_binary == 0) %>% pull(Proportion)
percent_no_diabetes3 <- round(prop_no_diabetes3*100)

# Proportion and percent of individuals who have diabetes (Table 3)
prop_diabetes3 <- balanced_target_result %>% filter(Diabetes_binary == 1) %>% pull(Proportion)
percent_diabetes3 <- round(prop_diabetes3*100)

knitr::kable(balanced_target_result)
```

After balancing, the distribution of individuals in both categories of `Diabetes_binary` is roughly `r percent_no_diabetes3` - `r percent_diabetes3` (@tbl-diabetes3). A summary of the class distribution before and after ROSE can be seen in @tbl-diabetes4 below.

```{r}
#| label: tbl-diabetes4
#| tbl-cap: Comparison of Class Distribution Before and After Balancing with ROSE

balanced_raw_comparision_df <- read_csv("/home/rstudio/work/output/balanced_raw_comparision_df.csv", show_col_types = FALSE)

knitr::kable(balanced_raw_comparision_df)
```

The balanced dataset, `balanced_raw_diabetes_df`, will be saved in the `/work/data/processed/` directory and then split into training (`diabetes_train`) and testing (`diabetes_test`) sets for machine learning.

### 3.3. EDA - Feature Selection and Visualization

*Analysis Workflow:*

```{r}
library(dplyr)

# Note: Rudimentary code for testing, NOT meant to be final
training_data <- readRDS("~/work/data/processed/diabetes_train.RDS")

bmi_bins <- c(-Inf, 18.5, 25, 30, 35, 40, Inf)
bmi_labels <- c(1, 2, 3, 4, 5, 6)

binned_df <- training_data %>%
  mutate(BinnedBMI = cut(BMI, breaks = bmi_bins, labels = bmi_labels, right = FALSE)) %>%
  select(-BMI)
```
Using every feature in the dataset would not be optimal as having a large number of features in our model could lead to a risk of overfitting, difficulty in interpretation and increased computation time. Thus, we will select only a subset of relevant predictors from the 21 total risk factors in the dataset.

First, bar plots are created below to visually determine correlation between each categorical variable and the target variable `Diabetes_binary`. For `BMI`, a density plot was created due to this feature being numerical.

![Distribution of Diabetes Binary by Various Variables](../output/combined_plots.png){#fig-eda1}

Of the 21 features, it appears that `HighBP`, `HighChol`, `CholCheck`, `Stroke`, `HeartDiseaseorAttack`, `HvyAlcoholConsump`, `DiffWalk`, `Age`, `Education`, `Income`, and `GenHlth` provide the most obvious differences in distribution between their categories and the target (@fig-eda1). 

The `BMI` feature will be binned into discrete categories corresponding to the CDC's BMI categories (https://www.cdc.gov/bmi/adult-calculator/bmi-categories.html). @tbl-diabetes111 below shows how the BMI feature is binned.

```{r}
#| label: tbl-diabetes111
#| tbl-cap: CDC BMI Categories and Corresponding Binned BMI value
# Note: Rudimentary code for testing, NOT meant to be final. Data taken from CDC https://www.cdc.gov/bmi/adult-calculator/bmi-categories.html
bmi_categories <- c(
  "Underweight", 
  "Healthy Weight", 
  "Overweight", 
  "Class 1 Obesity", 
  "Class 2 Obesity", 
  "Class 3 Obesity (Severe Obesity)"
)

bmi_ranges <- c(
  "Less than 18.5", 
  "18.5 to less than 25", 
  "25 to less than 30", 
  "30 to less than 35", 
  "35 to less than 40", 
  "40 or greater"
)

# Create the table (data frame)
bmi_table <- data.frame(
  "BMI_Category" = bmi_categories,
  "BMI_Range" = bmi_ranges,
  "BinnedBMI_Value" = 1:6
)

# View the table
knitr::kable(bmi_table)

```

To further narrow down relevant predictors, we conducted independent chi-squared tests to determine whether a significant relationship exists between each categorical variable and the target variable `Diabetes_binary`.

```{r}
#| label: tbl-diabetes5
#| tbl-cap: Chi-squared Statistic, Degrees of Freedom, p-value, and Cramér’s V sorted in Descending Order

# Arrange the results by p-value (from smallest to largest)
cramer_chi_results_sorted <- read_csv("/home/rstudio/work/output/cramer_chi_results_sorted.csv", show_col_types = FALSE)

knitr::kable(cramer_chi_results_sorted)
```

All chi-squared tests yielded very small p-values (\< 0.05), indicating significant relationships between each feature and the target (@tbl-diabetes5). However, there is a key limitation to this test: It only tests for statistical significance (i.e., whether an association exists) but does not indicate the strength of that association. In large datasets, chi-squared tests can produce extremely small p-values even for weak associations, making it difficult to determine their practical importance.

To address this, we employed Cramér's V which provides a standardized measure of the strength of association between each categorical variable and the target variable. Cramér's V quantifies how strong the association is on a scale from 0 (no association) to 1 (perfect association). To utilize Cramér's V, both the variables of interest and the target variable should be categorical. More than two unique values are also allowed for the categorical variables [@StatsTest2020].

We defined a Cramér's V value greater than 0.25 to be strongly associated with the target and sufficient to include the corresponding feature in our model. This value is supported in the literature; @Akoglu2018's correlation coefficient guide suggests that a value above 0.25 can be interpreted as a very strong relationship between the variables compared. Similarly, @Dai2021 considered Cramér's V values greater than 0.25 as reflecting a very strong association in their clinical study. Using this threshold, we selected the following features to use in our model: `GenHlth`, `HighBP`, `Age`, `HighChol`, and `DiffWalk` (@tbl-diabetes5).

### 3.4. Classification Analysis

*Analysis Workflow:*

Given the large quantity of binary features and with the task being for classification, a logistic regression model `lr_mod` will be required. The v-fold cross-validation split `vfold_cv()` is created in preparation for cross-validation of the dataset, to provide a more effective estimate of the model performance. `lr_recipe` is created to apply one-hot encoding to the categorical features, and normalization to all numerical variables.

`lambda_grid` is created as a grid of hyperparameters for tuning with the cross-validation set with penalty terms, while `tune_grid()` is used for hyperparameter tuning, specifically tuned for higher recall, as the consequences of false negatives for this task would be more severe. The metric that maximizes recall is selected for the finalized workflow `lasso_tuned_wflow`.

### 3.5. Result of Analysis - Visualization

*Analysis Workflow:*

The results of our LASSO regression predictions include `lasso_preds`, which provides the predictions for each row, `lasso_probs`, which provides the probability of each classification for each row, and `lasso_metrics`, which displays the following metrics in @tbl-diabetes6:

-   `sens`: Sensitivity; true positive rate
-   `spec`: Specificity; true negative rate
-   `ppv`: Positive predictive value; precision
-   `npv`: Negative predictive value
-   `accuracy`: Accuracy for all predictions
-   `recall`: Recall; true positive rate
-   `f_meas`: F-measure; harmonic mean of precision and recall
-   `roc_auc_value`: ROC AUC value; measure of the model's effectiveness in distinguishing between classes.

```{r}
#| label: tbl-diabetes6
#| tbl-cap: Classification Metrics for Lasso Model on Test Set

lasso_metrics <- read_csv("/home/rstudio/work/output/lasso_metrics.csv", show_col_types = FALSE)

# Pulls the recall metric
recall_value <- lasso_metrics %>% filter(.metric == "recall") %>% pull(.estimate)
# Recall metric in percentage
recall_percent <- round(recall_value*100)

# Pulls the sens metric for the False Negative rate
sens_value <- lasso_metrics %>% filter(.metric == "sens") %>% pull(.estimate)

# Pulls the auc metric
auc_value <- lasso_metrics %>% filter(.metric == "roc_auc") %>% pull(.estimate)

knitr::kable(lasso_metrics)
```

The ROC curve and confusion matrix are visualised below (@fig-roc2, @fig-cm3).

![ROC Curve for Lasso Model](../output/roc_curve.png){#fig-roc2 width="65%"}

```{r}
cm_df <- read_csv("/home/rstudio/work/output/cm_df.csv", show_col_types = FALSE)

# TP, TN, FP, FN values from confusion matrix
TP <- cm_df %>% filter(Truth == 1, Prediction == 1) %>% pull(Freq)
TN <- cm_df %>% filter(Truth == 0, Prediction == 0) %>% pull(Freq)
FP <- cm_df %>% filter(Truth == 0, Prediction == 1) %>% pull(Freq)
FN <- cm_df %>% filter(Truth == 1, Prediction == 0) %>% pull(Freq)

recall_calc <- TP / (TP + FN)
fnr_calc <- FN / (TP + FN)

# False negative rate in percentage
fnr_percent <- round(fnr_calc*100)
```

![Confusion Matrix for Lasso Model](../output/cm_plot.png){#fig-cm3 width="65%"}

The confusion matrix displays the quantity of each type of prediction result; The model predicts `r format(TP, scientific = FALSE)` true positive, `r format(TN, scientific = FALSE)` true negative, `r FN` false negative and `r FP` false positive cases (@fig-cm3).

Thus, the recall for the model can be calculated as $\frac{\text{True Positives}}{\text{True Positives} + \text{False Negatives}}$ = `r recall_calc` (7 s.f.). This is the same result as shown in @tbl-diabetes6.

The false negative rate can be calculated as: $\frac{\text{False Negatives}}{\text{True Positives} + \text{False Negatives}}$ = `r fnr_calc` (7 s.f.). This is the same result as calculating (1 - `sens`) from @tbl-diabetes6.

From @tbl-diabetes6, the values of particular interest are the `recall` score of `r recall_value` (7 s.f.) and the `roc_auc_value` of `r auc_value` (7 s.f.).

## 4. Discussion

Our model achieved a recall score of `r recall_value` (7 s.f.) on the test set, implying that about `r recall_percent`% of all positive instances of diabetes were correctly classified by the LASSO regression model (@tbl-diabetes6). This suggests that the model is relatively effective at identifying individuals who are at risk of developing diabetes. Additionally, the model achieved an area under the ROC curve of `r auc_value` (7 s.f.) on the test set (@tbl-diabetes6). Since the AUC is above 0.5, this indicates that the model can discriminate between diabetic and non-diabetic cases better than random guessing (@fig-cm3).

We expected the model to perform better than random guessing, which it did achieve. Additionally, we aimed to minimize false negatives which is particularly important in healthcare diagnoses where a false negative case can have serious consequences. For example, a false negative would indicate that the model predicts the patient to not develop diabetes even though they do. This may lead to the patient not getting the treatment or care they need, potentially resulting in health complications and even death. The model had a false negative rate around `r fnr_percent`% which is concerning as it indicates a significant risk for missing positive cases leading to unfavourable patient outcomes.

Our model was optimized for recall through hyperparameter tuning, with cross-validation used to evaluate model performance during the process. However, despite our results, the model falls short of what is expected in clinical applications. For example, more complex models in the literature which incorporate advanced feature selection techniques [@Alhussan2023] or Generative Adversarial Networks (GANs) [@Feng2023] can achieve recall and AUC scores upwards of 97%. Our findings serve as a proof of concept for the feasibility of classification models to predict the risk of diabetes based on publicly available health data. Since our model did relatively well compared to random guessing, this indicates potential correlations between health indicators and the likelihood of developing diabetes. With this information, people might be more aware of their health and lifestyle choices. They may be inclined to work harder to reduce cholesterol levels, manage high blood pressure, keep alcohol consumption under control and maintain a healthy lifestyle. Thus, this may help decrease the global mortality rate from diabetes through early interventions and lifestyle changes.

Future directions could include how we can improve our classification method to more accurately predict the risk of diabetes in patients. We can explore more rigorous feature selection techniques or implement other machine learning models such as boosted trees to improve our classification performance. In the context of the healthcare system, we can look to integrate classification models into the diagnosis process to help detect diabetes early to improve patient outcomes.

## 5. References
